{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boleydl/lee02328/miniconda3/envs/AVIT2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from diffusers import AutoencoderKL, UNet2DModel, DDPMScheduler\n",
    "#from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SSTDatasetWithMask(Dataset):\n",
    "    def __init__(self, sst_data, masks, sequence_length=6):\n",
    "        \"\"\"\n",
    "        SST Dataset with masks to generate (context frames, target frame, mask).\n",
    "\n",
    "        Args:\n",
    "            sst_data (torch.Tensor): SST data of shape (time, lat, lon).\n",
    "            masks (torch.Tensor): Masks of the same shape as sst_data.\n",
    "            sequence_length (int): Number of frames to condition on.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sst_data = sst_data\n",
    "        self.masks = masks\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sst_data.size(0) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.sst_data[idx:idx + self.sequence_length + 1]\n",
    "        mask = self.masks[idx + self.sequence_length].unsqueeze(0)\n",
    "        return frames, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusionTrainer:\n",
    "    def __init__(self, vae, unet, scheduler, optimizer):\n",
    "        self.vae = vae\n",
    "        self.unet = unet\n",
    "        self.scheduler = scheduler\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, dataloader, num_epochs=5, device=\"cuda\"):\n",
    "        print(\"Start train\", flush = True)\n",
    "        self.vae.to(device).eval()  # VAE stays in eval mode\n",
    "        self.unet.to(device)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for context_frames, mask in dataloader:\n",
    "                print(context_frames.shape, flush = True)\n",
    "\n",
    "                # Move data to device\n",
    "                context_frames = context_frames.to(device)  # Shape: (B, 4, H, W)\n",
    "                target_frame = context_frames[:, 3:, :, :]\n",
    "                context_frames = context_frames[:, :3, :, :]\n",
    "                \n",
    "                \n",
    "                print(context_frames.shape, flush = True)\n",
    "                print(target_frame.shape, flush = True)\n",
    "\n",
    "                # Encode images into latent space\n",
    "                with torch.no_grad():\n",
    "                    latent_contexts = self.vae.encode(context_frames).latent_dist.sample()\n",
    "                    print(latent_contexts.shape, flush = True)\n",
    "                    latent_target = self.vae.encode(target_frame).latent_dist.sample()\n",
    "                    print(latent_target.shape, flush = True)\n",
    "                \n",
    "                # Add noise to target latent\n",
    "                noise = torch.randn_like(latent_target)\n",
    "                timesteps = torch.randint(0, self.scheduler.num_train_timesteps, (target_frame.size(0),), device=device)\n",
    "                noisy_latent_target = self.scheduler.add_noise(latent_target, noise, timesteps)\n",
    "\n",
    "\n",
    "                input_val = torch.cat([latent_contexts, noisy_latent_target], dim = 1)\n",
    "                \n",
    "                print(input_val.shape, flush = True)\n",
    "\n",
    "                # Predict noise using the UNet\n",
    "                outputs = self.unet(input_val, timesteps).sample\n",
    "\n",
    "                # Compute loss\n",
    "                loss = (nn.MSELoss(reduction=\"none\")(outputs, noise)* mask).mean()\n",
    "                \n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load processed SST data and masks\n",
    "    sst_data = np.load('processed_sst_data.npy')  # Shape: (time, lat, lon)\n",
    "    masks = np.load('sst_masks.npy')  # Shape: (time, lat, lon)\n",
    "\n",
    "    # Ensure the data and masks match\n",
    "    assert sst_data.shape == masks.shape, \"Data and masks must have the same shape.\"\n",
    "\n",
    "    sst_data_tensor = torch.tensor(sst_data, dtype=torch.float32)\n",
    "    masks_tensor = torch.tensor(masks, dtype=torch.float32)\n",
    "    \n",
    "    # Dataset and DataLoader\n",
    "    sequence_length = 3  # Conditioning on the last 6 frames\n",
    "    dataset = SSTDatasetWithMask(sst_data_tensor, masks_tensor, sequence_length=sequence_length)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Load modeling components\n",
    "    vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "    \n",
    "    unet = UNet2DModel(\n",
    "        sample_size=64,  # Latent space resolution\n",
    "        in_channels=4,   # Channel size for the noisy frame\n",
    "        out_channels=1   # Channel size for the predicted clean frame\n",
    "    )\n",
    "    \n",
    "    # Diffusion Scheduler\n",
    "    scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = LatentDiffusionTrainer(vae, unet, scheduler, optimizer)\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train(dataloader, num_epochs=10, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print(\"Start\", flush = True)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
